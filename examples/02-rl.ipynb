{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving reinforcement learning problems using pgpelib\n",
    "\n",
    "In this example, we are going to solve the `CartPole-v1` environment.\n",
    "\n",
    "In addition to the `PGPE` class in its core namespace, `pgpelib` provides utilities for expressing policies (feed-forward neural network policies and linear policies are supported). This tutorial demonstrates those utility classes as well.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgpelib import PGPE\n",
    "from pgpelib.policies import LinearPolicy, MLPPolicy\n",
    "from pgpelib.restore import to_torch_module\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the environment to solve below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a policy object.\n",
    "\n",
    "A policy object:\n",
    "\n",
    "- stores a [PyTorch](https://pytorch.org/) module (in the form of a feed-forward neural network, or a linear transformation) to represent the structure of the policy;\n",
    "- provides ability to fill the parameters of the module from a parameter vector (so that solutions evolved by PGPE can be loaded);\n",
    "- can run an agent using the policy (with the loaded parameters) in a [gym](https://gym.openai.com/) environment.\n",
    "\n",
    "This policy object will serve as our fitness function.\n",
    "In more details, our goal is to maximize the total reward we get from the gym environment, so, we are looking for a parameter vector which, when loaded into the policy object, returns the highest amount of total reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pgpelib.policies.MLPPolicy at 0x7fa948419860>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = MLPPolicy(\n",
    "    \n",
    "    # The name of the environment in which the policy will be tested:\n",
    "    env_name=ENV_NAME,\n",
    "    \n",
    "    # Number of hidden layers:\n",
    "    num_hidden=1,\n",
    "    \n",
    "    # Size of a hidden layer:\n",
    "    hidden_size=8,\n",
    "    \n",
    "    # Activation function to be used in the hidden layers:\n",
    "    hidden_activation='tanh',\n",
    "    \n",
    "    # Whether or not to do online normalization on the observations\n",
    "    # received from the environments.\n",
    "    # The default is True, and using observation normalization\n",
    "    # can be very helpful.\n",
    "    # In this tutorial, we set it to False just to keep things simple.\n",
    "    observation_normalization=False\n",
    ")\n",
    "\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set an initial solution (initial parameter vector) for PGPE to start exploring from.\n",
    "In this case, we start from a zero-filled vector, its length being the number of parameters required by the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.zeros(policy.get_parameters_count(), dtype='float32')\n",
    "x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we initialize our PGPE solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pgpelib.pgpe.PGPE at 0x7fa93ecd90f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgpe = PGPE(\n",
    "    \n",
    "    # We are looking for solutions whose lengths are equal\n",
    "    # to the number of parameters required by the policy:\n",
    "    solution_length=policy.get_parameters_count(),\n",
    "    \n",
    "    # Population size:\n",
    "    popsize=250,\n",
    "    \n",
    "    # Initial mean of the search distribution:\n",
    "    center_init=x0,\n",
    "    \n",
    "    # Learning rate for when updating the mean of the search distribution:\n",
    "    center_learning_rate=0.075,\n",
    "    \n",
    "    # Optimizer to be used for when updating the mean of the search\n",
    "    # distribution, and optimizer-specific configuration:\n",
    "    optimizer='clipup',\n",
    "    optimizer_config={'max_speed': 0.15},\n",
    "    \n",
    "    # Initial standard deviation of the search distribution:\n",
    "    stdev_init=0.08,\n",
    "    \n",
    "    # Learning rate for when updating the standard deviation of the\n",
    "    # search distribution:\n",
    "    stdev_learning_rate=0.1,\n",
    "    \n",
    "    # Limiting the change on the standard deviation:\n",
    "    stdev_max_change=0.2,\n",
    "    \n",
    "    # Solution ranking (True means 0-centered ranking will be used)\n",
    "    solution_ranking=True,\n",
    "    \n",
    "    # dtype is expected as float32 when using the policy objects\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "pgpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run our evolution loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1   median score: 10.0\n",
      "Iteration: 2   median score: 10.0\n",
      "Iteration: 3   median score: 10.0\n",
      "Iteration: 4   median score: 10.0\n",
      "Iteration: 5   median score: 10.0\n",
      "Iteration: 6   median score: 10.0\n",
      "Iteration: 7   median score: 10.0\n",
      "Iteration: 8   median score: 13.0\n",
      "Iteration: 9   median score: 22.0\n",
      "Iteration: 10   median score: 29.0\n",
      "Iteration: 11   median score: 34.0\n",
      "Iteration: 12   median score: 44.0\n",
      "Iteration: 13   median score: 47.5\n",
      "Iteration: 14   median score: 54.0\n",
      "Iteration: 15   median score: 62.5\n",
      "Iteration: 16   median score: 70.0\n",
      "Iteration: 17   median score: 82.0\n",
      "Iteration: 18   median score: 80.0\n",
      "Iteration: 19   median score: 103.0\n",
      "Iteration: 20   median score: 134.5\n",
      "Iteration: 21   median score: 202.0\n",
      "Iteration: 22   median score: 261.0\n",
      "Iteration: 23   median score: 382.0\n",
      "Iteration: 24   median score: 475.0\n",
      "Iteration: 25   median score: 500.0\n",
      "Iteration: 26   median score: 500.0\n",
      "Iteration: 27   median score: 500.0\n",
      "Iteration: 28   median score: 500.0\n",
      "Iteration: 29   median score: 500.0\n",
      "Iteration: 30   median score: 500.0\n",
      "Iteration: 31   median score: 500.0\n",
      "Iteration: 32   median score: 500.0\n",
      "Iteration: 33   median score: 500.0\n",
      "Iteration: 34   median score: 500.0\n",
      "Iteration: 35   median score: 500.0\n",
      "Iteration: 36   median score: 500.0\n",
      "Iteration: 37   median score: 500.0\n",
      "Iteration: 38   median score: 500.0\n",
      "Iteration: 39   median score: 500.0\n",
      "Iteration: 40   median score: 500.0\n",
      "Iteration: 41   median score: 500.0\n",
      "Iteration: 42   median score: 500.0\n",
      "Iteration: 43   median score: 500.0\n",
      "Iteration: 44   median score: 500.0\n",
      "Iteration: 45   median score: 500.0\n",
      "Iteration: 46   median score: 500.0\n",
      "Iteration: 47   median score: 500.0\n",
      "Iteration: 48   median score: 500.0\n",
      "Iteration: 49   median score: 500.0\n",
      "Iteration: 50   median score: 500.0\n"
     ]
    }
   ],
   "source": [
    "# Number of iterations\n",
    "num_iterations = 50\n",
    "\n",
    "# The main loop of the evolutionary computation\n",
    "for i in range(1, 1 + num_iterations):\n",
    "\n",
    "    # Get the solutions from the pgpe solver\n",
    "    solutions = pgpe.ask()\n",
    "\n",
    "    # The list below will keep the fitnesses\n",
    "    # (i-th element will store the reward accumulated by the\n",
    "    # i-th solution)\n",
    "    fitnesses = []\n",
    "    \n",
    "    for solution in solutions:\n",
    "        # For each solution, we load the parameters into the\n",
    "        # policy and then run it in the gym environment,\n",
    "        # by calling the method set_params_and_run(...).\n",
    "        # In return we get our fitness value (the accumulated\n",
    "        # reward), and num_interactions (an integer specifying\n",
    "        # how many interactions with the environment were done\n",
    "        # using these policy parameters).\n",
    "        fitness, num_interactions = policy.set_params_and_run(solution)\n",
    "        \n",
    "        # In the case of this example, we are only interested\n",
    "        # in our fitness values, so we add it to our fitnesses list.\n",
    "        fitnesses.append(fitness)\n",
    "    \n",
    "    # We inform our pgpe solver of the fitnesses we received,\n",
    "    # so that the population gets updated accordingly.\n",
    "    pgpe.tell(fitnesses)\n",
    "    \n",
    "    print(\"Iteration:\", i, \"  median score:\", np.median(fitnesses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get the center point (i.e. mean) of the search distribution as our final solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19697863,  0.15743393,  0.2706258 ,  0.44188738, -0.65611017,\n",
       "        0.14537652,  0.09155902,  0.55707943, -0.06450157,  0.72363424,\n",
       "        0.32821342,  0.03987094, -0.09272053, -0.04438904, -0.05690874,\n",
       "       -0.53247833, -0.3564978 , -1.0647632 , -1.5106467 , -1.3660367 ,\n",
       "       -0.29835108, -0.45291892,  0.6627035 , -0.01135479, -0.25734568,\n",
       "       -0.02408281,  0.09201899,  0.95454407, -0.28973174, -0.42773312,\n",
       "        0.2055804 , -0.10689969,  0.23113649,  0.4962121 , -0.04484191,\n",
       "       -0.08292229, -0.11028722, -0.32896033,  0.8065649 ,  0.06629611,\n",
       "       -0.13041477,  0.02464531, -0.39663786,  0.21919973,  1.3402436 ,\n",
       "       -0.45583156, -0.1102507 ,  0.025219  ,  0.47419178,  0.05519801,\n",
       "        0.9163134 ,  0.01721925, -0.52789515, -0.16436481,  0.02202819,\n",
       "       -0.09143799,  0.04324111,  0.17192318], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_solution = pgpe.center.copy()\n",
    "center_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to test this final solution in the gym environment, and visualize its behavior.\n",
    "\n",
    "For visualization of the agent, we instantiate a gym environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<CartPoleEnv<CartPole-v1>>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the parameters of our final solution into the policy, and then convert that policy object to a PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=8, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.set_parameters(center_solution)\n",
    "\n",
    "net = to_torch_module(policy)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to manually test our final policy, by using gym and PyTorch.\n",
    "Below is the loop for generating a trajectory of our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare the cumulative_reward variable, which will accumulate\n",
    "# all the rewards we get from the environment\n",
    "cumulative_reward = 0.0\n",
    "\n",
    "# Reset the environment, and get the observation of the initial\n",
    "# state into a variable.\n",
    "observation = env.reset()\n",
    "\n",
    "# Visualize the initial state\n",
    "env.render()\n",
    "\n",
    "# Main loop of the trajectory\n",
    "while True:\n",
    "\n",
    "    # We pass the observation vector through the PyTorch module\n",
    "    # and get an action vector\n",
    "    with torch.no_grad():\n",
    "        action = net(\n",
    "            torch.as_tensor(observation, dtype=torch.float32)\n",
    "        ).numpy()\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        # If the action space of the environment is Box\n",
    "        # (that is, continuous), then the action vector returned\n",
    "        # by the policy is what we will send to the environment.\n",
    "        # This is the case for continuous control environments\n",
    "        # like 'Humanoid-v2', 'Walker2d-v2', 'HumanoidBulletEnv-v0'.\n",
    "        interaction = action\n",
    "    elif isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        # If the action space of the environment is Discrete,\n",
    "        # then the returned vector is in this form:\n",
    "        #   [ suggestionForAction0, suggestionForAction1, ... ]\n",
    "        # We get the index of the action that has the highest\n",
    "        # suggestion value, and that index is what we will\n",
    "        # send to the environment.\n",
    "        # This is the case for discrete-actioned environments\n",
    "        # like 'CartPole-v1'.\n",
    "        interaction = int(np.argmax(action))\n",
    "    else:\n",
    "        assert False, \"Unknown action space\"\n",
    "\n",
    "    observation, reward, done, info = env.step(interaction)\n",
    "    env.render()\n",
    "\n",
    "    cumulative_reward += reward\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "cumulative_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
